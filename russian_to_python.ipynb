{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602c3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from deep_translator import GoogleTranslator\n",
    "import inflect\n",
    "import nltk\n",
    "from langid import classify\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24352690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь KEYWORDS_1 имеет вид \"название особого токена\" : \"его 'перевод'\"; тут лежат токены, \n",
    "# которые просто заменяются на свои питоновские аналоги\n",
    "KEYWORDS_1 = {\"функция\": \"def\", \"класс\": \"class\", \"вернуть\": \"return\", \"добавлять\": \"append\", \n",
    "              \"для\": \"for\", \"в\": \"in\", \"во\": \"in\", \"не\": \" not\", \"инициализация\": \"__init__\", \"если\": \"if\", \n",
    "              \"иначе\": \"else\", \"случайный\": \"random.random()\", \"объект\": \"self\", \"предел\": \"range\", \"от\": \"(\", \n",
    "              \"до\": \",\", \"целый\": \"int\"}\n",
    "# словарь KEYWORDS_2 также имеет вид \"название особого токена\" : \"его 'перевод'\"; тут лежат токены, \n",
    "# которые не только заменяются на свои питоновские аналоги, но и требуют после себя наличие открытой скобки\n",
    "KEYWORDS_2 = {\"длина\": \"len\", \"минимум\": \"min\", \"максимум\": \"max\"}\n",
    "# словарь OPERATIONS имеет вид \"название бинарной операции\" : \"её символ\"\n",
    "OPERATIONS = {\"равно\": \"=\", \"равный\": \"=\", \"сумма\": \"+\", \"разность\": \"-\", \"поделить\": \"/\", \"больше\": \">\", \n",
    "              \"мало\": \"<\", \"произведение\": \"*\"}\n",
    "# массив NO_WHITESPACE_AFTER содержит символы, после которых пробел не ставится\n",
    "NO_WHITESPACE_AFTER = [\"[\", \"(\"]\n",
    "# массив NO_WHITESPACE_BEFORE содержит символы, перед которыми пробел не ставится\n",
    "NO_WHITESPACE_BEFORE = [\"(\", \"]\", \")\", \",\", \"[\", \".append\", \":\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5e367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция parse_operation принимает на вход:\n",
    "#\n",
    "# stemmed_tokens: массив токенов после стемминга\n",
    "#\n",
    "# token_analyses: массив информации о токенах (если токены англоязычные, не текстовые или смешанные \n",
    "# (текстово-символьные), то на данном месте хранятся сами токены, если же токен состоит из русских букв, то на \n",
    "# данном месте хранится словарь из MyStem().analyze()); здесь и далее структура подобного массива одинакова\n",
    "# \n",
    "# operation: строка-ключ бинарной операции из словаря OPERATIONS (см. выше)\n",
    "#\n",
    "# OPERATIONS: словарь с бинарными операциями (см. выше)\n",
    "#\n",
    "# что делает: заменяет в массиве токенов словесное описание операции на символ операции, избавляется от \n",
    "# вспомогательных слов вроде \"на\" в случае \"произведение a на b\"\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# модифицированный массив токенов\n",
    "#\n",
    "# модифицированный массив информации о токенах\n",
    "\n",
    "def parse_operation(stemmed_tokens, token_analyses, operation, OPERATIONS):\n",
    "    if operation == \"равно\" or operation == \"равный\" or operation == \"больше\" or operation == \"мало\":\n",
    "        oper_ix = stemmed_tokens.index(operation)\n",
    "        l1 = stemmed_tokens[:oper_ix]\n",
    "        r1 = stemmed_tokens[oper_ix + 1:]\n",
    "        l2 = token_analyses[:oper_ix]\n",
    "        r2 = token_analyses[oper_ix + 1:]\n",
    "        return l1 + [OPERATIONS[operation]] + r1, l2 + [OPERATIONS[operation]] + r2\n",
    "    elif operation == \"поделить\":\n",
    "        oper_ix = stemmed_tokens.index(operation)\n",
    "        l1 = stemmed_tokens[:oper_ix]\n",
    "        r1 = stemmed_tokens[oper_ix + 2:]\n",
    "        l2 = token_analyses[:oper_ix]\n",
    "        r2 = token_analyses[oper_ix + 2:]\n",
    "        return l1 + [OPERATIONS[operation]] + r1, l2 + [OPERATIONS[operation]] + r2\n",
    "    elif operation == \"произведение\":\n",
    "        on_ix = stemmed_tokens.index(\"на\")\n",
    "        oper_ix = stemmed_tokens.index(operation)\n",
    "        l1 = stemmed_tokens[1:oper_ix]\n",
    "        s1 = stemmed_tokens[oper_ix + 1:on_ix]\n",
    "        r1 = stemmed_tokens[on_ix + 1:]\n",
    "        l2 = token_analyses[1:oper_ix]\n",
    "        s2 = token_analyses[oper_ix + 1:on_ix]\n",
    "        r2 = token_analyses[on_ix + 1:]\n",
    "        return l1 + s1 + [OPERATIONS[operation]] + r1, l2 + s2 + [OPERATIONS[operation]] + r2 \n",
    "    else:\n",
    "        and_ix = stemmed_tokens.index(\"и\")\n",
    "        oper_ix = stemmed_tokens.index(operation)\n",
    "        l1 = stemmed_tokens[1:oper_ix]\n",
    "        s1 = stemmed_tokens[oper_ix + 1:and_ix]\n",
    "        r1 = stemmed_tokens[and_ix + 1:]\n",
    "        l2 = token_analyses[1:oper_ix]\n",
    "        s2 = token_analyses[oper_ix + 1:and_ix]\n",
    "        r2 = token_analyses[and_ix + 1:]\n",
    "        return l1 + s1 + [OPERATIONS[operation]] + r1, l2 + s2 + [OPERATIONS[operation]] + r2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a746b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция parse_script принимает на вход:\n",
    "#\n",
    "# filename: имя входного файла с русским текстом программы\n",
    "#\n",
    "# что делает: генерирует на основе входного файла выходной файл output.py\n",
    "#\n",
    "# что возвращает: \n",
    "# ---\n",
    "\n",
    "def parse_script(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    g = open(\"output.py\", \"w\")\n",
    "    classes = {}\n",
    "    class_methods = {}\n",
    "    for line in f:\n",
    "        if len(line.strip()) == 0:\n",
    "            g.write(\"\\n\")\n",
    "        else:\n",
    "            preparsed_string, token_info = preparse_string(line)\n",
    "            parsed_string, token_info, classes, class_methods = collect_functions_and_classes(preparsed_string, \n",
    "                                                                                              token_info, classes, \n",
    "                                                                                              class_methods)\n",
    "            parsed_string = preparse_variables(parsed_string, token_info, classes, class_methods)\n",
    "            final_string = handle_whitespace(parsed_string, NO_WHITESPACE_AFTER, NO_WHITESPACE_BEFORE)\n",
    "            g.write(final_string + \"\\n\")\n",
    "    f.close()\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f541cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция preparse_string принимает на вход:\n",
    "#\n",
    "# string: строка\n",
    "#\n",
    "# что делает: разбивает строку на токены, к которым применяется стемминг; получает информацию о токенах; \n",
    "# обрабатывает словесное описание бинарных операций на основе словаря OPERATION, ключевых слов на основе словарей\n",
    "# KEYWORDS_1, KEYWORDS_2; вырезает вспомогательные слова; расставляет скобки\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# preparsed_string: массив токенов\n",
    "# token_info: массив информации о токенах\n",
    "\n",
    "def preparse_string(string):\n",
    "    tokens_to_be_ignored = [\"число\", \"к\", \"из\"]\n",
    "    brace_balance = 0\n",
    "    i = 0\n",
    "    preparsed_string = []\n",
    "    while i < len(list(string)) and (list(string)[i] == \" \" or list(string)[i] == \"\\t\"):\n",
    "        if len(preparsed_string) > 0:\n",
    "            preparsed_string[0] += list(string)[i]\n",
    "        else:\n",
    "            preparsed_string.append(list(string)[i])\n",
    "        string = string[1:]\n",
    "    tokens = nltk.word_tokenize(string)\n",
    "    mystem = Mystem()\n",
    "    stemmed_tokens = [mystem.lemmatize(token)[0] if token.isalpha() else token for token in tokens]\n",
    "    token_analyses = [mystem.analyze(token)[0]['analysis'] if 'analysis' in mystem.analyze(token)[0].keys() \n",
    "                      and classify(token)[0] != \"en\" else token for token in tokens]\n",
    "    token_analyses = [summary[0] if type(summary) == list and len(summary) > 0 else summary \n",
    "                      for summary in token_analyses]\n",
    "    operation_flag = False\n",
    "    for operation in OPERATIONS.keys():\n",
    "        if operation in stemmed_tokens:\n",
    "            if not operation_flag:\n",
    "                stemmed_tokens, token_analyses = parse_operation(preparsed_string + stemmed_tokens, \n",
    "                                                                 preparsed_string + token_analyses, \n",
    "                                                                 operation, OPERATIONS)\n",
    "                operation_flag = True\n",
    "            else:\n",
    "                stemmed_tokens, token_analyses = parse_operation(stemmed_tokens, token_analyses, \n",
    "                                                                 operation, OPERATIONS)\n",
    "    token_info = []\n",
    "    for i in range(len(stemmed_tokens)):\n",
    "        token = stemmed_tokens[i]\n",
    "        if token in KEYWORDS_1.keys():\n",
    "            preparsed_string.append(KEYWORDS_1[token])\n",
    "            token_info.append(KEYWORDS_1[token])\n",
    "            if token == \"от\":\n",
    "                brace_balance += 1\n",
    "        elif token in KEYWORDS_2.keys():\n",
    "            preparsed_string.append(KEYWORDS_2[token])\n",
    "            preparsed_string.append(\"(\")\n",
    "            token_info.append(KEYWORDS_2[token])\n",
    "            token_info.append(\"(\")\n",
    "            brace_balance += 1\n",
    "        elif token in [\" \"*n for n in range(1, 20)] or token in tokens_to_be_ignored:\n",
    "            continue\n",
    "        else:\n",
    "            preparsed_string.append(token)\n",
    "            token_info.append(token_analyses[i])\n",
    "    if brace_balance > 0:\n",
    "        if preparsed_string[-1] == \":\":\n",
    "            preparsed_string.insert(-1, \")\")\n",
    "            token_info.insert(-1, \")\")\n",
    "        else:\n",
    "            preparsed_string.append(\")\")\n",
    "            token_info.append(\")\")\n",
    "        if brace_balance > 1:\n",
    "            for i in range(len(preparsed_string) - 2, 0, -1):\n",
    "                if preparsed_string[i].isalpha():\n",
    "                    preparsed_string.insert(i + 1, \")\")\n",
    "                    token_info.insert(i + 1, \")\")\n",
    "                    break\n",
    "    return preparsed_string, token_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b697c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция translate_function_and_class принимает на вход:\n",
    "#\n",
    "# name_list: массив токенов для перевода - название функции, класса или метода класса\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# что делает: с помощью гугл-переводчика переводит название функции, класса или метода класса; здесь\n",
    "# рассматривается только несколько случаев: если в массиве больше трёх элементов или только один элемент, то\n",
    "# перевод пословный, если в массиве два элемента, то рассматривается случай двух существительных (второе \n",
    "# существительное выступает в качестве определения первого, при переводе они меняются местами)\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# строку-имя функции, класса или метода класса, при необходимости склеенное с помощью нижних подчёркиваний\n",
    "\n",
    "def translate_function_and_class(name_list, t_info):\n",
    "    pt = GoogleTranslator(source=\"russian\", target=\"english\")\n",
    "    translated = []\n",
    "    if len(name_list) >= 3 or len(name_list) == 1:\n",
    "        for word in name_list:\n",
    "            translated.append(pt.translate(word))\n",
    "        return \"_\".join(translated)\n",
    "    else:\n",
    "        morphology = [t[\"gr\"] for t in t_info]\n",
    "        if \"S\" in morphology[0] and \"S\" in morphology[1]:\n",
    "            # сущ (ед), сущ (ед) / сущ (мн) -> бутылка воды / список людей\n",
    "            if \"ед\" in morphology[0]:\n",
    "                return \"_\".join([pt.translate(name_list[1]), pt.translate(name_list[0])])\n",
    "        else:\n",
    "            return \"_\".join(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75774811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция collect_functions_and_classes принимает на вход:\n",
    "#\n",
    "# preparsed_string: массив токенов\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# classes: словарь вида \"название класса\": \"перевод названия класса\"\n",
    "#\n",
    "# class_methods: словарь вида \"название метода класса\": \"перевод названия метода класса\"\n",
    "#\n",
    "# что делает: в строках-инициализациях функций, классов и методов классов находит и перевод с помощью функции\n",
    "# translate_function_and_class их названия; при необходимости дополняет словари classes и class_methods (изначально\n",
    "# они пустые)\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# parsed_string: массив токенов\n",
    "#\n",
    "# token_info: массив информации о токенах\n",
    "#\n",
    "# classes: словарь вида \"название класса\": \"перевод названия класса\"\n",
    "#\n",
    "# class_methods: словарь вида \"название метода класса\": \"перевод названия метода класса\"\n",
    "\n",
    "def collect_functions_and_classes(preparsed_string, t_info, classes, class_methods):\n",
    "    parsed_string = []\n",
    "    token_info = []\n",
    "    if preparsed_string[0] == \"def\":\n",
    "        parsed_string.append(\"def\")\n",
    "        token_info.append(\"def\")\n",
    "        function_name = []\n",
    "        i = 1\n",
    "        while i < len(preparsed_string) and preparsed_string[i] != \"(\":\n",
    "            function_name.append(preparsed_string[i])\n",
    "            i += 1\n",
    "        translated = translate_function_and_class(function_name, t_info[1:i])\n",
    "        parsed_string.append(translated) \n",
    "        parsed_string.append(\"(\")\n",
    "        parsed_string += preparsed_string[i + 1:]\n",
    "        token_info.append(translated)\n",
    "        token_info.append(\"(\")\n",
    "        token_info += t_info[i + 1:]\n",
    "    elif preparsed_string[0] in [\" \"*n for n in range(1, 20)] and preparsed_string[1] == \"def\":\n",
    "        parsed_string.append(preparsed_string[0])\n",
    "        parsed_string.append(preparsed_string[1])\n",
    "        token_info.append(preparsed_string[0])\n",
    "        token_info.append(preparsed_string[1])\n",
    "        class_function_name = []\n",
    "        i = 2\n",
    "        while i < len(preparsed_string) and preparsed_string[i] != \"(\":\n",
    "            class_function_name.append(preparsed_string[i])\n",
    "            i += 1\n",
    "        class_function_name_one_word = \"_\".join(class_function_name)\n",
    "        if class_function_name_one_word not in class_methods.keys():\n",
    "            class_methods[class_function_name_one_word] = {}\n",
    "        translated = translate_function_and_class(class_function_name, t_info[1:i - 1])\n",
    "        class_methods[class_function_name_one_word] = translated\n",
    "        parsed_string.append(translated)\n",
    "        parsed_string.append(\"(\")\n",
    "        parsed_string += preparsed_string[i + 1:]\n",
    "        token_info.append(translated)\n",
    "        token_info.append(\"(\")\n",
    "        token_info += t_info[i:]\n",
    "    elif preparsed_string[0] == \"class\":\n",
    "        parsed_string.append(\"class\")\n",
    "        token_info.append(\"class\")\n",
    "        class_name = []\n",
    "        i = 1\n",
    "        while i < len(preparsed_string) and preparsed_string[i] != \":\":\n",
    "            class_name.append(preparsed_string[i])\n",
    "            i += 1\n",
    "        class_name_one_word = \"_\".join(class_name)\n",
    "        if class_name_one_word not in classes.keys():\n",
    "            classes[class_name_one_word] = {}\n",
    "        translated = translate_function_and_class(class_name, t_info[1:i])\n",
    "        translated = translated[0].upper() + translated[1:]\n",
    "        classes[class_name_one_word] = translated\n",
    "        parsed_string.append(translated)\n",
    "        parsed_string.append(\":\")\n",
    "        token_info.append(translated)\n",
    "        token_info.append(\":\")\n",
    "    else:\n",
    "        parsed_string = preparsed_string\n",
    "        if preparsed_string[0] in [\" \"*n for n in range(1, 20)]:\n",
    "            token_info = [preparsed_string[0]] + t_info\n",
    "        else:\n",
    "            token_info = t_info\n",
    "    return parsed_string, token_info, classes, class_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e62e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция handle_list принимает на вход:\n",
    "#\n",
    "# parsed_string: массив токенов\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# что делает: если в строке нет информации о работе с элементами массивов, то не делает ничего; иначе обрабатывает\n",
    "# добавление элемента в массив (в т.ч. с помощью информации о падежах слов) и вызов элемента массива\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# parsed_string: массив токенов\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "\n",
    "def handle_list(parsed_string, t_info):\n",
    "    if \"элемент\" in parsed_string: # a[i]\n",
    "        if \"append\" not in parsed_string: # array.append(a[i])\n",
    "            ix = parsed_string.index(\"элемент\")\n",
    "            elem = parsed_string[ix - 3]\n",
    "            for i in range(1, len(parsed_string) - ix):\n",
    "                i0 = i\n",
    "                if not parsed_string[ix + i].isalpha():\n",
    "                    break\n",
    "            list_name_ixs = [ix + i for i in range(1, i0 + 1)]\n",
    "            list_name = [parsed_string[ix + i] for i in range(1, i0 + 1)]\n",
    "            if not list_name[-1].isalpha():\n",
    "                list_name_ixs = list_name_ixs[:-1]\n",
    "                list_name = list_name[:-1]\n",
    "            preparsed_string = parsed_string[:ix - 4] + list_name + [\"[\", elem, \"]\"] + \\\n",
    "                               parsed_string[max(list_name_ixs) + 1:]\n",
    "            token_info = t_info[:ix - 4] + list(np.array(t_info)[list_name_ixs]) + [\"[\", elem, \"]\"] + \\\n",
    "                         t_info[max(list_name_ixs) + 1:]\n",
    "            return preparsed_string, token_info\n",
    "        else: # a[i] = ...\n",
    "            ix = parsed_string.index(\"элемент\")\n",
    "            append_ix = parsed_string.index(\"append\")\n",
    "            elem = parsed_string[ix - 3]\n",
    "            first_list_name_ixs = [append_ix + i for i in range(1, ix - 4) \n",
    "                                   if parsed_string[append_ix + i].isalpha()]\n",
    "            first_list_name = [parsed_string[append_ix + i] for i in range(1, ix - 4) \n",
    "                               if parsed_string[append_ix + i].isalpha()]\n",
    "            second_list_name_ixs = [ix + i for i in range(1, len(parsed_string) - ix) \n",
    "                                    if parsed_string[ix + i].isalpha()]\n",
    "            second_list_name = [parsed_string[ix + i] for i in range(1, len(parsed_string) - ix) \n",
    "                                if parsed_string[ix + i].isalpha()]\n",
    "            preparsed_string = parsed_string[:append_ix] + first_list_name + [\".append\", \"(\"] + \\\n",
    "                               second_list_name + [\"[\", elem, \"]\", \")\"]\n",
    "            token_info = t_info[:append_ix] + list(np.array(t_info)[first_list_name_ixs]) + \\\n",
    "                         [\".append\", \"(\"] + list(np.array(t_info)[second_list_name_ixs]) + [\"[\", elem, \"]\", \")\"]\n",
    "            return preparsed_string, token_info\n",
    "    elif \"append\" in parsed_string and \"элемент\" not in parsed_string: # array.append(element)\n",
    "        append_ix = parsed_string.index(\"append\")\n",
    "        elem_name_ixs = [append_ix + i for i in range(1, len(parsed_string) - append_ix) \n",
    "                         if parsed_string[append_ix + i].isalpha() and \"вин,ед\" in t_info[append_ix + i][\"gr\"]]\n",
    "        elem_name = [parsed_string[append_ix + i] for i in range(1, len(parsed_string) - append_ix) \n",
    "                         if parsed_string[append_ix + i].isalpha() and \"вин,ед\" in t_info[append_ix + i][\"gr\"]]\n",
    "        preparsed_string = parsed_string[:append_ix] + parsed_string[append_ix + 1:min(elem_name_ixs)] + \\\n",
    "                           [\".append\", \"(\"] + elem_name + [\")\"]\n",
    "        token_info = t_info[:append_ix] + t_info[append_ix + 1:min(elem_name_ixs)] + [\".append\", \"(\"] + \\\n",
    "                     list(np.array(t_info)[elem_name_ixs]) + [\")\"]\n",
    "        return preparsed_string, token_info\n",
    "    else:\n",
    "        return parsed_string, t_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e72f030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция handle_classes принимает на вход:\n",
    "#\n",
    "# var_name: массив токенов-имя переменной, функции и т.д.\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# classes: словарь вида \"название класса\": \"перевод названия класса\"\n",
    "#\n",
    "# class_methods: словарь вида \"название метода класса\": \"перевод названия метода класса\"\n",
    "#\n",
    "# что делает: обрабатывает имена классов и методов классов в строках, где нет их инициализации; для примера, из \n",
    "# массива ['первая', 'координата', 'город'] делает массив ['город.первая', 'координата']\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# var_name: новый массив токенов-имя переменной, функции и т.д.\n",
    "#\n",
    "# t_info: новый массив информации о токенах\n",
    "#\n",
    "# class_flag: флаг, который равен True в случае если работа производилась с именем класса (чтобы потом написать его\n",
    "# название с большой буквы), False иначе\n",
    "\n",
    "def handle_classes(var_name, t_info, classes, class_methods):\n",
    "    class_flag = False\n",
    "    token_info = []\n",
    "    if len(var_name) > 2 and var_name[-1] in classes.keys():\n",
    "        end = var_name[-1] + \".\" + var_name[0]\n",
    "        end_token = str(t_info[-1]) + '.' + str(t_info[0])\n",
    "        token_info = [end_token] + t_info[1:-1]\n",
    "        return [end] + var_name[1:-1], token_info, class_flag\n",
    "    elif len(var_name) == 1 and var_name[0] in classes.keys():\n",
    "        class_flag = True\n",
    "        return var_name, t_info, class_flag\n",
    "        \n",
    "    elif len(var_name) > 1 and var_name[0] in class_methods.keys() and var_name[1] in classes.keys():\n",
    "        end = var_name[-1] + \".\" + var_name[0]\n",
    "        end_token = str(t_info[-1]) + \".\" + str(t_info[0])\n",
    "        token_info = t_info[1:-1] + [end_token]\n",
    "        return var_name[1:-1] + [end], token_info, class_flag\n",
    "    else:\n",
    "        return var_name, t_info, class_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25ca655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция translate_two_words принимает на вход:\n",
    "#\n",
    "# name_list: массив токенов из двух элементов для перевода - название переменной, функции и т.д.\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# что делает: с помощью гугл-переводчика переводит название переменной, функции и т.д.; здесь рассматривается\n",
    "# несколько случаев: случай двух существительных (второе существительное выступает в качестве определения первого, \n",
    "# при переводе они меняются местами), случай прилагательного или порядкового числительного и существительного (при\n",
    "# переводе порядок прямой); решается проблема, когда при переводе из одного слова возникает несколько\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# строку-имя переменной, функции и т.д., при необходимости склеенное с помощью нижних подчёркиваний\n",
    "\n",
    "def translate_two_words(name_list, t_info):\n",
    "    pt = GoogleTranslator(source=\"russian\", target=\"english\")\n",
    "    morphology = [t[\"gr\"] for t in t_info]\n",
    "    if \"S\" in morphology[0] and \"S\" in morphology[1]:\n",
    "        # сущ (ед), сущ (ед) / сущ (мн) -> бутылка воды, список людей\n",
    "        if \"ед\" in morphology[0]:\n",
    "            l = pt.translate(name_list[1])\n",
    "            if \"the\" in l:\n",
    "                l = l[4:]\n",
    "            r = pt.translate(name_list[0])\n",
    "            if \"the\" in r:\n",
    "                r = r[4:]\n",
    "            return \"_\".join([l, r])\n",
    "        # сущ (мн), сущ (ед) / сущ (мн) -> бутылки воды, списки людей\n",
    "        elif \"мн\" in morphology[0]:\n",
    "            l = pt.translate(name_list[1])\n",
    "            if \"the\" in l:\n",
    "                l = l[4:]\n",
    "            r = pt.translate(name_list[0])\n",
    "            if \"the\" in r:\n",
    "                r = r[4:]\n",
    "            engine = inflect.engine()\n",
    "            return \"_\".join([l, engine.plural(r)])\n",
    "        else:\n",
    "            return \"_\".join(name_list)\n",
    "    if \"S\" in morphology[1] and (\"A\" in morphology[0] or \"ANUM\" in morphology[0]):\n",
    "        # прил, сущ / порядк числ, сущ -> новый дом, первый квартал\n",
    "        l = pt.translate(name_list[0])\n",
    "        if \"the\" in l:\n",
    "            l = l[4:]\n",
    "        r = pt.translate(name_list[1])\n",
    "        if \"the\" in r:\n",
    "            r = r[4:]\n",
    "        return \"_\".join([l, r])\n",
    "    else:\n",
    "        return \"_\".join(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbb4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция translate_variables принимает на вход:\n",
    "#\n",
    "# name_list: массив токенов для перевода - название переменной, функции и т.д.\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# что делает: с помощью гугл-переводчика переводит название переменной, функции и т.д.; здесь рассматривается\n",
    "# несколько случаев: если в массиве больше трёх элементов или только один элемент, то перевод пословный, если в \n",
    "# массиве два элемента, то применяется функция translate_two_words; особым образом обрабатываются классы и их \n",
    "# методы\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# строку-имя переменной, функции и т.д., при необходимости склеенное с помощью нижних подчёркиваний\n",
    "\n",
    "def translate_variables(name_list, t_info):\n",
    "    pt = GoogleTranslator(source=\"russian\", target=\"english\")\n",
    "    translated = []\n",
    "    if len(name_list) >= 3 or len(name_list) == 1:\n",
    "        for word in name_list:\n",
    "            tr = pt.translate(word)\n",
    "            if \"the\" not in tr:\n",
    "                translated.append(tr)\n",
    "            else:\n",
    "                translated.append(tr[4:])\n",
    "        return \"_\".join(translated)\n",
    "    else:\n",
    "        if \".\" in name_list[0]:\n",
    "            point_ix = name_list[0].index(\".\")\n",
    "            point_ix_token = t_info[0].index(\".\")\n",
    "            new_t_info = [eval(t_info[0][point_ix_token + 1:])] + [t_info[1]]\n",
    "            right = translate_two_words([name_list[0][point_ix + 1:]] + [name_list[1]], new_t_info)\n",
    "            left = pt.translate(name_list[0][:point_ix])\n",
    "            if \"the\" not in left:\n",
    "                return left + \".\" + right\n",
    "            else:\n",
    "                return left[4:] + \".\" + right\n",
    "        elif \".\" in name_list[1]:\n",
    "            point_ix = name_list[1].index(\".\")\n",
    "            point_ix_token = t_info[1].index(\".\")\n",
    "            new_t_info = [t_info[0]] + [eval(t_info[1][:point_ix_token])]\n",
    "            left = translate_two_words([name_list[0]] + [name_list[1][:point_ix]], new_t_info)\n",
    "            right = pt.translate(name_list[1][point_ix + 1:])\n",
    "            if \"the\" not in right:\n",
    "                return left + \".\" + right\n",
    "            else:\n",
    "                return left + \".\" + right[4:]\n",
    "        else:\n",
    "            return translate_two_words(name_list, t_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d794dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция preparse_variables принимает на вход:\n",
    "#\n",
    "# parsed_string: массив токенов\n",
    "#\n",
    "# t_info: массив информации о токенах\n",
    "#\n",
    "# classes: словарь вида \"название класса\": \"перевод названия класса\"\n",
    "#\n",
    "# class_methods: словарь вида \"название метода класса\": \"перевод названия метода класса\"\n",
    "#\n",
    "# что делает: находит в строке название переменной, функции и т.д., переводит его с помощью функции \n",
    "# translate_variables; особым образом обрабатываются названия с self\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# preparsed_string: массив токенов\n",
    "\n",
    "def preparse_variables(parsed_string, t_info, classes, class_methods):\n",
    "    preparsed_string = []\n",
    "    token_info = []\n",
    "    var_name = []\n",
    "    var_ixs = []\n",
    "    p_string, tk_info = handle_list(parsed_string, t_info)\n",
    "    for i in range(len(p_string)):\n",
    "        if p_string[i].isalpha() and classify(p_string[i])[0] != \"en\":\n",
    "            var_name.append(p_string[i])\n",
    "            var_ixs.append(i)\n",
    "        elif p_string[i] == \"self\":\n",
    "            var_name.append(p_string[i])\n",
    "        else:\n",
    "            if len(var_name) > 0:\n",
    "                if \"self\" not in var_name:\n",
    "                    tt_info = list(np.array(tk_info)[var_ixs])\n",
    "                    new_var_name, var_info, class_flag = handle_classes(var_name, tt_info, classes, class_methods)\n",
    "                    translated = translate_variables(new_var_name, var_info)\n",
    "                    if not class_flag:\n",
    "                        preparsed_string.append(translated)\n",
    "                        preparsed_string.append(p_string[i])\n",
    "                        var_name = []\n",
    "                        var_ixs = []\n",
    "                    else:\n",
    "                        preparsed_string.append(translated[0].upper() + translated[1:])\n",
    "                        preparsed_string.append(p_string[i])\n",
    "                        var_name = []\n",
    "                        var_ixs = []\n",
    "                elif len(var_name) > 1:\n",
    "                    tt_info = list(np.array(tk_info)[var_ixs])\n",
    "                    new_var_name, var_info, class_flag = handle_classes(var_name[:-1], tt_info, classes, \n",
    "                                                                        class_methods)\n",
    "                    translated = translate_variables(new_var_name, var_info)\n",
    "                    preparsed_string.append(\"self.\" + translated)\n",
    "                    preparsed_string.append(p_string[i])\n",
    "                    var_name = []\n",
    "                    var_ixs = []\n",
    "                else:\n",
    "                    preparsed_string.append(\"self\")\n",
    "                    preparsed_string.append(p_string[i])\n",
    "                    var_name = []\n",
    "                    var_ixs = []\n",
    "            else:\n",
    "                preparsed_string.append(p_string[i])\n",
    "    if len(var_name) > 0:\n",
    "        if \"self\" not in var_name:\n",
    "            tt_info = list(np.array(tk_info)[var_ixs])\n",
    "            new_var_name, var_info, class_flag = handle_classes(var_name, tt_info, classes, class_methods)\n",
    "            translated = translate_variables(new_var_name, var_info)\n",
    "            preparsed_string.append(translated)\n",
    "            var_name = []\n",
    "            var_ixs = []\n",
    "        elif len(var_name) > 1:\n",
    "            tt_info = list(np.array(tk_info)[var_ixs])\n",
    "            new_var_name, var_info, class_flag = handle_classes(var_name[:-1], tt_info, classes, class_methods)\n",
    "            translated = translate_variables(new_var_name, var_info)\n",
    "            preparsed_string.append(\"self.\" + translated)\n",
    "            var_name = []\n",
    "            var_ixs = []\n",
    "        else:\n",
    "            preparsed_string.append(\"self\")\n",
    "            var_name = []\n",
    "            var_ixs = []\n",
    "    return preparsed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba754726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция handle_whitespace принимает на вход:\n",
    "#\n",
    "# string: массив токенов\n",
    "#\n",
    "# NO_WHITESPACE_AFTER: см. выше\n",
    "#\n",
    "# NO_WHITESPACE_BEFORE: см. выше\n",
    "#\n",
    "# что делает: правильно расставляет пробелы между токенами\n",
    "#\n",
    "# что возвращает: \n",
    "#\n",
    "# финальную строку\n",
    "\n",
    "def handle_whitespace(string, NO_WHITESPACE_AFTER, NO_WHITESPACE_BEFORE):\n",
    "    final_string = [string[0]]\n",
    "    for i in range(1, len(string)):\n",
    "        if string[i] in NO_WHITESPACE_BEFORE:\n",
    "            if string[i - 1] in NO_WHITESPACE_AFTER:\n",
    "                final_string += [string[i]]\n",
    "            else:\n",
    "                if string[i] == \"[\" and string[i - 1] == \"=\":\n",
    "                    final_string += [\" \", string[i]]\n",
    "                else:\n",
    "                    final_string += [string[i]]\n",
    "        elif string[i] not in NO_WHITESPACE_BEFORE:\n",
    "            if string[i - 1] in NO_WHITESPACE_AFTER:\n",
    "                final_string += [string[i]]\n",
    "            else:\n",
    "                final_string += [\" \", string[i]]\n",
    "    return \"\".join(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d76237b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем\n",
    "parse_script(\"genetic_algorithm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa9c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем, сгенерированный скрипт запускается\n",
    "! python output.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654912dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
